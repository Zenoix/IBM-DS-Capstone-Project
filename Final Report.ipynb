{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the Problem? - An Analysis of New York Housing Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As part of IBM's professional certificate for data science, I was required to complete this capstone project where I had to analyse housing and building complaints in the state of New York. I was given information such as metadata of each complaint made for a house and the details about the buildings.\n",
    "\n",
    "\n",
    "### Context of the Project\n",
    "\n",
    ">The people of New York use the 311 system to report complaints about the non-emergency problems to local authorities. Of those complaints, the Department of Housing Preservation and Development of New York City is the agency that processes 311 complaints that are related to housing and buildings.\n",
    "\n",
    ">In the last few years, the number of 311 complaints coming to the Department of Housing Preservation and Development has increased significantly. Although these complaints are not necessarily urgent, the large volume of complaints and the sudden increase is impacting the overall efficiency of operations of the agency.\n",
    "\n",
    ">Therefore, the Department of Housing Preservation and Development has approached your organization to help them manage the large volume of 311 complaints they are receiving every year. \n",
    "\n",
    "\n",
    "By analysing existing data from the New York Department of Housing Preservation and Development, I attempted to answer these questions posed by the department:\n",
    "- What type of complaint should be focused on first?\n",
    "- Should the department focus on any particular geographical area (borough, zip codes, or street) for the determined complaint type?\n",
    "- Is there any obvious relationship between the characteristics of a house or building and the complaint type?\n",
    "- Can a predictive model be created for future possible complaints of the determined complainted type?\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "### Prediction Model Results\n",
    "For binary target (complaint made or not):\n",
    "\n",
    "- Logistic regression: Jaccard score of 0.989, F1 score of 0.99\n",
    "\n",
    "For predicting the number of complaints:\n",
    "\n",
    "1. K-Nearest Neighbors: accuracy score of 0.97\n",
    "2. Multiple linear regression: Residual sum of squares of 25923.79, R^2/coeffient of determination of 0.08\n",
    "\n",
    "### Complaint Type Result\n",
    "From analysing the data, it was discovered that complaints relating to heating occurred the most with over 2,149,424 complaints compared to the next highest with only 711,141 complaints in comparison. From this, it was determined that heating complaints were going to be focused on to answer the questions that the department provided.\n",
    "\n",
    "![\"Complaint Types\"](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/Complaint%20Types.png)\n",
    "\n",
    "### Geographical Results\n",
    "Regarding the locations where these complaints occurred, the Bronx had the most amount of complaints with Brooklyn having the second most followed by Manhatten, Queens, and then Staten Island. \n",
    "\n",
    "![\"Geographical\"](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/Boroughs%20Graph.png)\n",
    "\n",
    "The 5 zip codes with the highest complaint numbers were: \n",
    "\n",
    "1. 11226: 69041 complaints\n",
    "2. 10467: 66073 complaints\n",
    "3. 10458: 65372 complaints\n",
    "4. 10468: 58190 complaints\n",
    "5. 10453: 57818 complaints\n",
    "\n",
    "and the 5 streets with the most complaints were:\n",
    "\n",
    "1. GRAND CONCOURSE: 37863 complaints\n",
    "2. BROADWAY: 24484 complaints\n",
    "3. OCEAN AVENUE: 18716 complaints\n",
    "4. MORRIS AVENUE: 16409 complaints\n",
    "5. ARDEN STREET: 15963 complaints\n",
    "\n",
    "Based on the number of heating complaints for each borough, I would suggest the Department to focus on **The Bronx**.\n",
    "Looking at the number of complaints per zip code, the zip codes with the most complaints are **11226** located in Brooklyn, and 10467 located in the Bronx.\n",
    "Lastly, going off the streets with the most amount of complaints, the **Grand Concourse** has the highest amount of complaints with Broadway second.\n",
    "\n",
    "### Analysis of Correlations Between Building Characteristics and Complaints\n",
    "As the Bronx has the highest number of heating complaints out of all the boroughs I focussed on the information for houses and buildings that are located in the Bronx. The data about the building characteristics contained information such as lot numbers, building area, number of floors, and the year the buildings were built. From the information provided, I decided to use two approaches to find a relationship between characteristics and complaints:\n",
    "\n",
    "1. Use a binary approach where the features will either end in a complaint being made (represented by a 1) or not (represented by a 0)\n",
    "2. Using total number of complaints for each street to analyse correlation\n",
    "\n",
    "To find out if there is any correlation, I calculated the Pearson correlation and p-values for the features of the buildings and the target. Lastly, I created a correlation matrix to help visualise the Pearson correlations.\n",
    "\n",
    "#### Approach 1:\n",
    "From the correlation data frame, and the correlation matrix, we see that the feature with the highest correlation to a heating complaint being made is the year that the building was built (Pearson=-0.7553). The next highest is the number of floors the house has (Pearson=-0.7313). I believe that the results of the p-values are inconclusive as they are all zero or extremely close to zero. From this, we may infer that this binary approach may not be the method for answering our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|              | Lot Number  | Lot Area    | Building Area | Residential Area | Office Area | Retail Area | Num of Buildings | Num of Floors | Lot Depth    | Building Depth | Year Built | Year 1st Altered | Floor Area Ratio | Max Residential FAR | Max Commercial FAR | Max Facility FAR |\n",
    "|--------------|-------------|-------------|---------------|------------------|-------------|-------------|------------------|---------------|--------------|----------------|------------|------------------|------------------|---------------------|--------------------|------------------|\n",
    "| Pearson Corr | 0.0135295   | 0.0113194   | -0.079132     | -0.0981778       | 0.00396574  | -0.0695622  | -0.253903        | -0.731387     | 0.0277289    | -0.602128      | -0.755379  | -0.218871        | -0.421666        | 0.0663305           | 0.0513572          | 0.0679223        |\n",
    "| P-Value      | 3.18555e-49 | 5.62946e-35 | 0             | 0                | 1.54197e-05 | 0           | 0                | 0             | 9.60683e-201 | 0              | 0          | 0                | 0                | 0                   | 0                  | 0                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Correlation Matrix](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/CorrMatrixBinary.png \"Correlation Matrix 1\")\n",
    "\n",
    "#### Approach 2:\n",
    "From the Pearson correlation, we see that once again the year the building was built has the highest correlation to the number of complaints made with a Pearson correlation value of -0.277962 followed by the building depth and the number of floors. Also, the p-values of this approach seem to be a bit better than the binary approach which may suggest that this may be a better method of finding the correlation between features and complaints made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|              | Lot Number | Lot Area   | Building Area | Residential Area | Office Area | Retail Area | Num of Buildings | Num of Floors | Lot Depth  | Building Depth | Year Built | Year 1st Altered | Floor Area Ratio | Max Residential FAR | Max Commercial FAR | Max Facility FAR |\n",
    "|--------------|------------|------------|---------------|------------------|-------------|-------------|------------------|---------------|------------|----------------|------------|------------------|------------------|---------------------|--------------------|------------------|\n",
    "| Pearson Corr | 0.00608823 | 0.00124213 | -0.00878389   | -0.00795726      | 0.00177427  | -0.00562281 | -0.0429668       | -0.117689     | 0.0142087  | -0.1181        | -0.277962  | -0.0233512       | -0.0476101       | 0.0282175           | 0.0231703          | 0.0265687        |\n",
    "| P-Value      | 0.0685153  | 0.710158   | 0.00858445    | 0.0172735        | 0.595516    | 0.0925013   | 7.40908e-38      | 1.68606e-273  | 2.12414e-5 | 2.07135e-275   | 0          | 2.79631e-12      | 4.2979e-46       | 3.05826e-17         | 4.10718e-12        | 1.85436e-15      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Correlation Matrix](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/CorrMatrixAmounts.png \"Correlation Matrix 2\")\n",
    "\n",
    "From both approaches, we see that some features may be useful in predicting whether a complaint will be made or how many complaints will be made from the features of a building. Using the binary approach, we find that 3 features have a high correlation of a complaint being made. These are the year that the building was built (Pearson=-0.7553), the number of floors the house has (Pearson=-0.7313), and the depth of the building (Pearson=-0.6021). Although we have three features that have a high correlation, we see that the vast majority of p-values are zeros with the rest all being extremely close to 0. This either means that all the features are statistically significant or that the binary approach is not suitable for the dataset we have. From the second approach, we do not have any features that are highly correlated to the number of complaints made for a building. The highest correlation we have is once again the year the building was built (Pearson=-0.277962) followed by the depth of the building, and the number of floors it has. However, the p-values of the second approach are much more promising as there do not seem to be any abnormalities and most of the values are less than or equal to 0.5.\n",
    "\n",
    "Overall, for both approaches, the year that the building was built seems to have the highest correlation to complaints being made. Although it is out of the scope of the project, we could think about a possible reason for this correlation such as older buildings were made with less advanced techniques and technology compared to more recent buildings or maybe older buildings have had more time to deteriorate that haven't been maintained.\n",
    "\n",
    "### Model Building\n",
    "\n",
    "Once again, I used the same 2 approaches to build machine learning models:\n",
    "\n",
    "1. Use a binary approach where the features will either end in a complaint being made (represented by a 1) or not (represented by a 0)\n",
    "2. Consider the total number of complaints for each street based on the building's features\n",
    "\n",
    "**Note: all the models were trained and tested using a 70-30 split to prevent underfitting and overfitting.**\n",
    "\n",
    "#### Approach 1:\n",
    "\n",
    "For the binary approach, I only created 1 predictive model. This was a logistic regression model to predict whether a complaint would be made for a building based on its features.\n",
    "To evaluate the model, I used the Jaccard Index which helps gauge the similarity of data sets, a confusion matrix, and the model's F1 score.\n",
    "\n",
    "##### Jaccard Index\n",
    "\n",
    "The Jaccard Index is a statistic used to understand the similarities between sample sets. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; if none of the predicted labels matches, it is 0.0. Our model has a Jaccard index of 0.9899171594785319 which is very good.\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "![Confusion Matrix](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/ConfusionMatrixBinary.png \"Confusion Matrix 1\")\n",
    "\n",
    "Looking at the confusion matrix, it seems that the model is very accurate at predicting when a complaint will be made and when a complaint will not be made. Out of 330,189 complaints being made, the model managed to accurately predict 329,094 of them which is about 99.67%. On the other hand, out of 26,232 complaints not being made, the model predicted that 2,257 complaints were made when they weren't. This means that the model managed to correctly predict about 91.39% of the buildings that did not make a complaint. Although this is lower than the true positive rate, it is still reasonably accurate.\n",
    "\n",
    "##### F1 score\n",
    "![F1 Score](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/F-1%20Score%20Logistic%20Reg.png \"F1 Score\")\n",
    "\n",
    "From wikipedia: *In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive.* Ideally, we want the F1 score to be as high as possible with 1 being the max which, in this case, our model has achieved  an F1 score of 0.99.\n",
    "\n",
    "\n",
    "#### Approach 2\n",
    "For the second approach of the model building, I created 2 predictive models, one a multiple linear regression and the other a k-nearest neighbours model.\n",
    "\n",
    "##### Multiple Linear Regression\n",
    "Using a Python package called scikit-learn, I created and fit the multiple linear regression model to the data set of housing characteristics and heating complaints. From this, the model's coefficients of the features and intercept were:\n",
    "- Coefficients:  [  0.14,   0.36,   1.81,  -0.72,   0.22,   0.46,  -0.34,  -2.7,   -1.22,  -1.1,\n",
    " -39.97,  -0.23,  -1.42,   8.8,   -1.18,  -1.85]\n",
    "- Intercept: 11.721307551851885\n",
    "\n",
    "To evaluate the regression model, I evaluated the model's residual sum of squares and variance score on the testing data. The residual sum of squares was 26923.79 which is considerably high which indicates a high amount of error in the multiple linear regression. The models R^2 or coefficient of determination is 0.08 which is low (1 is the highest) indicating that the model may not fit our data well enough. As the multiple linear regression underperforms in terms of both the residual sum of squares and variance score, we can assume that the model may not be effective as a predictive tool for future heating complaints.\n",
    "\n",
    "##### K-Nearest Neighbors\n",
    "For the K-Nearest Neighbors model, I first experimented with different values of K (from 1 to 9) to find a value with high accuracy. A graph of the values of K plotted against the model accuracy:\n",
    "\n",
    "![KNN](https://raw.githubusercontent.com/Zenoix/IBM-DS-Capstone-Project/master/Images/KNN.png \"K Values\")\n",
    "\n",
    "After building, fitting, and testing the KNN Model with a K value of nine, the accuracy on the testing data was 0.9696913281453625 which indicates that this model may be a better fit than the multiple linear regression.\n",
    "\n",
    "#### Model Building Conclusion\n",
    "\n",
    "For the first approach of a binary target (1 or 0), I only developed one model which was the logistic regression. This model had a Jaccard score of 0.989 and an F1 score of 0.99. These are both very high scores which indicate that the model may be useful as a predictive model to predict if a complaint will be made for any building.\n",
    "\n",
    "For the second approach of considering the total number of complaints per building, I developed two models: a multiple linear regression and a k-nearest neighbours model. For the multiple linear regression, the accuracy was very disappointing. It had a residual sum of squares of 26923.79 (the lower the better) and an R^2 of 0.08 (best score is 1). This indicates that it will most likely not be a good model to predict complaints. On the other hand, the KNN model did much better in its evaluation. Using a K value of 9, the model had an accuracy score of roughly 0.97. This means that the KNN model would probably be a better model to use to predict the number of complaints a building will make.\n",
    "\n",
    "Depending on the method the Department of Housing Preservation and Development wants to employ, it is possible to make a predictive model from the data they have. If they would like to use the binary approach, a logistic regression model will be a good fit for future predictions. On the other hand, if they would like to predict the total number of complaints for a building, then I would recommend the KNN model as I believe that it is a better fit than the multiple logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Used\n",
    "\n",
    "For this project, I was given 2 datasets to analyse in order to answer the questions given by the Department of Housing Preservation and Development. These were the 311 complaint dataset that contains information on complaints to the Department related to housing and building complaints, and the PLTO dataset which has information on every building's characteristics. I also found a coordinate data on the boundaries of New York boroughs that I used to help visualise the locations of a sample of heating complaints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 311 complaint dataset\n",
    "The dataset I used was already placed on an IBM server which is where I got it from. The original dataset is available at https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9 which can be downloaded by using the API. This dataset contains metadata of complaints made to the Department relating to housing and building problems. The 311 complaint dataset has 41 columns of information however, for this project, I only used a selection of these columns to analyse the data.\n",
    "These were: \n",
    "- Created Date - The date and time the complaint was made and entered into the 311 system\n",
    "- Closed Date - The date and time the complaint was closed by the department\n",
    "- Complaint Type - The category of complaint identifying the topic of the incident or condition\n",
    "- Location Type\t- Describes the type of location used in the address information\n",
    "- Incident Zip - Incident location zip code\n",
    "- Incident Address - House number of incident address provided by submitter\n",
    "- Street Name - Street name of incident address provided by the submitter\n",
    "- Address Type - Type of incident location information available\n",
    "- City - City of the incident location\n",
    "- Status - Status of the complaint\n",
    "- Resolution Description - \tDescribes the last action taken on the incident. May describe next or future steps\n",
    "- Borough - Borough of the incident location provided by the submitter\n",
    "- Latitude - Geo based Lat of the incident location\n",
    "- Longitude - Geo based Lat of the incident location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLUTO dataset for housing\n",
    "The PLUTO dataset which contains information on every building in New York City's characteristic was used to find whether complaints had any correlation with complaints being made. I obtained the data set from here: https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nyc_pluto_18v1.zip. \n",
    "This dataset contained a lot of information about individual buildings but the features I used in the analysis were:\n",
    "- Lot Number - The number of the tax lot\n",
    "- ZipCode - The zip code that the tax lot is located in\n",
    "- Address - The address of the tax lot\n",
    "- Lot Area - The total area of the tax lot in square feet\n",
    "- Building Area - The total gross building floor area in square feet\n",
    "- Residential Area - An estimate of the exterior dimensions of the portion of the structure(s) allocated for residential use\n",
    "- Office Area - An estimate of the exterior dimensions of the portion of the structure(s) allocated for office use\n",
    "- Retail Area - An estimate of the exterior dimensions of the portion of the structure(s) allocated for retail use\n",
    "- Num of Buildings - The number of buildings on the tax lot\n",
    "- Num of Floors - In the tallest building on the tax lot, the number of full and partial stories starting from the ground floor\n",
    "- Lot Depth - The tax lot's depth measured in feet\n",
    "- Building Depth - The buildingâ€™s depth, which is the effective perpendicular distance, measured in feet\n",
    "- Year Built - The year construction of the building was completed\n",
    "- Year 1st Altered - If a building has only been altered once, this value is the date of the alteration. If a building has been altered more than once, this value is the year of the second most recent alteration.\n",
    "- Floor Area Ratio - The Built Floor Area Ratio (FAR) is the total building floor area divided by the area of the tax lot\n",
    "- Max Residential FAR - The Maximum Allowable Residential Floor Area Ratio (FAR)\n",
    "- Max Commercial FAR - The Maximum Allowable Commercial Floor Area Ratio (FAR)\n",
    "- Max Facility FAR - The Maximum Allowable Community Facility Floor Area Ratio (FAR)\n",
    "- X Coord - The X coordinate of the XY coordinate pair which depicts the approximate location of the lot.\n",
    "- Y Coord - The Y coordinate of the XY coordinate pair which depicts the approximate location of the lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data on borough boundaries in New York\n",
    "This contained geographical data about the boundaries of the boroughs in New York in a geojson format. This was used to help with the analysis of the geographical effects on the number of complaints made. This data can be accessed from https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Which type of complaint should the Department of Housing Preservation and Development of New York City focus on first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the first question proposed by the department, I did some basic data exploration on the 311 complaints dataset first before starting cleaning and analysing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File 311_Service_Requests_from_2010_to_Present_min.csv does not exist: '311_Service_Requests_from_2010_to_Present_min.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6a27688e849c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mNY311_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"311_Service_Requests_from_2010_to_Present_min.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mNY311_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unnamed: 0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mNY311_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unique Key\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File 311_Service_Requests_from_2010_to_Present_min.csv does not exist: '311_Service_Requests_from_2010_to_Present_min.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "NY311_df = pd.read_csv(\"311_Service_Requests_from_2010_to_Present_min.csv\")\n",
    "NY311_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "NY311_df.set_index(\"Unique Key\", inplace=True)\n",
    "NY311_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "After importing the dataset, this is a sample of the 311 complaint dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NY311_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I wanted to gain some information about the dataset such as how many complaints are contained, what metadata we can analyse, and what complaint types are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"The shape of the dataset is:\", NY311_df.shape)\n",
    "print(\"Number of complaints:\", NY311_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"The information we have include:\\n\", list(NY311_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(NY311_df[\"Complaint Type\"].value_counts())\n",
    "print(\"\\nThere seems to be\", NY311_df[\"Complaint Type\"].value_counts().count(), \"complaint types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seemed as though there were 30 complaint types however, there also seemed to have duplicates/incorrect complaint types due to inconsistencies in capitalisation and formatting. As a result of this, I figured that the only cleaning of the data I needed to do was to merge complaint types and make the formatting and capitalisation consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "My first step of cleaning was to make all the values in the complaint type category fully capitalised as the values seem to have inconsistent capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NY311_df[\"Complaint Type\"] = NY311_df[\"Complaint Type\"].str.upper()\n",
    "print(\"After this step, the new complaint types are:\", list(NY311_df[\"Complaint Type\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to remove duplicate categories of complaint types. I merged `HEAT/HOT WATER` and `HEATING` together to form only one `HEATING` type, `CONSTRUCTION` and `GENERAL CONSTRUCTION` into `GENERAL CONSTRUCTION`, and `PAINT - PLASTER` with `PAINT/PLASTER` to `PAINT/PLASTER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "replace_categories = {\"HEAT/HOT WATER\": \"HEATING\", \n",
    "                      \"CONSTRUCTION\": \"GENERAL CONSTRUCTION\",\n",
    "                      \"PAINT - PLASTER\" : \"PAINT/PLASTER\"             \n",
    "                     } \n",
    "NY311_df[\"Complaint Type\"] = NY311_df[\"Complaint Type\"].replace(replace_categories)\n",
    "print(\"The new complaint types are:\", NY311_df[\"Complaint Type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has now been cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "The last thing to do is to analyse the 311 complaint dataset. To do this I first graphed out the amount of complaints for each complaint type, and then used 800,000 complaints as a threshold for whether the department should focus on a specific complaint type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x=NY311_df[\"Complaint Type\"].unique(), y=NY311_df[\"Complaint Type\"].value_counts())\n",
    "ax.set(xlabel=\"Complaint Type\", ylabel=\"Number of Complaints\")\n",
    "ax.set_title(\"Number of Complaints for each Complaint Type\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NY311_df[\"Complaint Type\"].value_counts() >= 800000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was only one complaint type that met the threshold which was **HEATING** complaints. From this, I decided that the Department of Housing Preservation and Development should focus on HEATING complaints and would be the focus of the rest of my analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Should the Department of Housing Preservation and Development of New York City focus on any particular set of boroughs, ZIP codes, or street (where the complaints are severe) for the specific type of complaints you identified in response to Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, these were the steps I followed:\n",
    "1. Create a new dataframe from the cleaned 311 dataset that only contained heating complaints and their corresponding geographical information\n",
    "2. Explore the frequency of complaints for each borough, zip code, and street\n",
    "3. Visualise the distribution of complaints on a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "This step was quite easy. I created a new dataframe and added data entries where the complaint type value was HEATING. I also kept information related to the geography of the complaint such as the zip code, address, street name, city, borough, latitude, and longitude. This is a snippet of the heating dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "heating_df = NY311_df[NY311_df[\"Complaint Type\"]==\"HEATING\"].reset_index()\n",
    "heating_df.drop([\"Created Date\", \"Unique Key\", \"Resolution Description\", \"Closed Date\", \"Location Type\", \"Address Type\"], axis=1, inplace=True)\n",
    "heating_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Next, I created tables for the number of complaints for each borough and graphed them in a bar graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for number of complaints for each borough\n",
    "borough_df=pd.DataFrame({\"Number of Complaints\": heating_df.groupby([\"Borough\"]).size()}).reset_index()\n",
    "borough_df[\"Borough\"] = borough_df[\"Borough\"].str.title()\n",
    "borough_df.drop(5, axis=0, inplace=True)\n",
    "borough_df.sort_values(by=\"Number of Complaints\", ascending=False, inplace=True)\n",
    "borough_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualise the dataframe\n",
    "\n",
    "ax = sns.barplot(x=\"Borough\", y=\"Number of Complaints\", data=borough_df)\n",
    "ax.set_title(\"Number of Heating Complaints per Borough\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table and graph, the Bronx has the most heating complaints followed by Brooklyn, Manhattan, Queens, and then Staten Island."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, I focused on finding the top 5 zip codes, streets, and addresses with the most heating complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "zip_df = pd.DataFrame({\"Number of Complaints\": heating_df.groupby([\"Incident Zip\"]).size()}).reset_index()\n",
    "zip_df.sort_values(by=\"Number of Complaints\", ascending=False, inplace=True)\n",
    "zip_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "street_df = pd.DataFrame({\"Number of Complaints\": heating_df.groupby([\"Street Name\"]).size()}).reset_index()\n",
    "street_df.sort_values(by=\"Number of Complaints\", ascending=False, inplace=True)\n",
    "street_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "address_df = pd.DataFrame({\"Number of Complaints\": heating_df.groupby([\"Incident Address\"]).size()}).reset_index()\n",
    "address_df.sort_values(by=\"Number of Complaints\", ascending=False, inplace=True)\n",
    "address_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "For the last part of the geographical analysis of the heating complaints, I decided to create a visualisation of the complaints on a map of the boroughs of New York. As the heating dataframe contains over two million entries, I decided on only plot a small sample of 1000 randomly selected complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "heating_loc_df = heating_df.dropna(axis=0).sample(n=1000)\n",
    "\n",
    "# import folium for visualition geographical data\n",
    "import folium\n",
    "\n",
    "# download the geojson file for the boroughs in New York City\n",
    "!wget --quiet https://data.cityofnewyork.us/resource/7t3b-ywvw.geojson -O boroughs.json\n",
    "borough_bounds = r\"boroughs.json\"\n",
    "\n",
    "# create choropleth map of NYC boroughs\n",
    "borough_map = folium.Map(location=[40.72, -73.9], tiles=\"cartodbpositron\", zoom_start=10)\n",
    "folium.Choropleth(\n",
    "    geo_data=borough_bounds,\n",
    "    name=\"choropleth\",\n",
    "    data=borough_df,\n",
    "    columns=[\"Borough\", \"Number of Complaints\"],\n",
    "    key_on='feature.properties.boro_name',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.6,\n",
    "    line_opacity=0.2\n",
    ").add_to(borough_map)\n",
    "\n",
    "# instantiate a feature group for the complaints in the dataframe\n",
    "complaints = folium.map.FeatureGroup()\n",
    "\n",
    "# for each complaint, plot the coordinates\n",
    "for lat, lng, in zip(heating_loc_df.Latitude, heating_loc_df.Longitude):\n",
    "    complaints.add_child(\n",
    "        folium.CircleMarker(\n",
    "            [lat, lng],\n",
    "            radius=5, # define how big you want the circle markers to be\n",
    "            color='purple',\n",
    "            fill=True,\n",
    "            fill_color='white',\n",
    "            fill_opacity=0.6\n",
    "        )\n",
    "    )\n",
    "\n",
    "# add the coordinates to the map\n",
    "borough_map.add_child(complaints)\n",
    "\n",
    "# If the map cannot be shown to you, access it through this file\n",
    "borough_map.save(\"Borough complaint map.html\")\n",
    "\n",
    "# Show the map\n",
    "borough_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Does the Complaint Type that you identified in response to question 1 have an obvious relationship with any particular characteristic or characteristics of the houses or buildings?\n",
    "To answer this question, I needed to import the PLUTO dataset that contains information about individual houses and buildings. For the analysis, I chose to use only a few columns of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "house_df = pd.read_csv(\"PLUTO_for_WEB/BX_18v1.csv\", usecols=[\"Address\", \"BldgArea\", \"BldgDepth\", \"BuiltFAR\", \"CommFAR\", \n",
    "                                                             \"FacilFAR\", \"Lot\", \"LotArea\", \"LotDepth\", \"NumBldgs\", \n",
    "                                                             \"NumFloors\", \"OfficeArea\", \"ResArea\", \"ResidFAR\", \"RetailArea\", \n",
    "                                                             \"YearBuilt\", \"YearAlter1\", \"ZipCode\", \"YCoord\",\"XCoord\"])\n",
    "\n",
    "column_names = {\"Lot\": \"Lot Number\", \"LotArea\": \"Lot Area\", \"BldgArea\": \"Building Area\", \n",
    "                         \"ResArea\": \"Residential Area\", \"OfficeArea\": \"Office Area\", \"RetailArea\": \"Retail Area\",\n",
    "                         \"NumBldgs\": \"Num of Buildings\", \"NumFloors\": \"Num of Floors\", \"LotDepth\": \"Lot Depth\",\n",
    "                         \"BldgDepth\": \"Building Depth\", \"YearBuilt\": \"Year Built\", \"YearAlter1\": \"Year 1st Altered\",\n",
    "                         \"BuiltFAR\": \"Floor Area Ratio\", \"ResidFAR\": \"Max Residential FAR\", \"CommFAR\": \"Max Commercial FAR\",\n",
    "                         \"FacilFAR\": \"Max Facility FAR\", \"XCoord\": \"X Coord\", \"YCoord\": \"Y Coord\"}\n",
    "\n",
    "house_df.rename(columns=column_names, inplace=True)\n",
    "\n",
    "house_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as the Bronx has the most heating complaints I created a new dataset for houses and buildings in the Bronx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bronx_heating_df = heating_df[heating_df[\"Borough\"]==\"BRONX\"]\n",
    "bronx_heating_df = bronx_heating_df.dropna(subset=[\"Street Name\", \"Incident Zip\"])\n",
    "bronx_heating_df.drop([\"Status\", \"Latitude\", \"Longitude\", \"City\", \"Borough\"], axis=1, inplace=True)\n",
    "bronx_heating_df.reset_index(drop=True, inplace=True)\n",
    "bronx_heating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse the correlation between heating complaints being made and the characteristics of buildings I employed two approaches:\n",
    "\n",
    "1. Use a binary approach where the features will either end in a complaint being made (represented by a 1) or not (represented by a 0)\n",
    "2. Using total number of complaints for each street to analyse correlation\n",
    "\n",
    "To find out if there is any correlation, I calculated the Pearson correlation and p-values for the features of the buildings and the target. Lastly, I created a correlation matrix to help visualise the Pearson correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1\n",
    "I first created a new dataframe that from the heating complaints and added a new column of ones to indicate the locations where a heating complaint was made. I also deleted the \"Complaint Type\" column as it was not needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_heating_df = bronx_heating_df\n",
    "binary_heating_df[\"Complaint Made\"] = np.ones(len(binary_heating_df), dtype=int)\n",
    "binary_heating_df.drop(\"Complaint Type\", axis=1, inplace=True)\n",
    "binary_heating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then merged this new dataframe with the dataframe containing building features by the corresponding addresses. This means that the two dataframes are now merged together where rows with the same address will be aligned together. After that I deleted columns that were no longer needed (these were geographical data which was only used for the merging of datasets). Lastly, I inserted zeros in the rows where there was no complaint made. In the end, I had one dataframe that had the building address as the ID, a columns of ones and zeros for complaints made, and building features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(binary_heating_df, house_df, how=\"right\", left_on=[\"Street Name\", \"Incident Zip\"], right_on=[\"Address\", \"ZipCode\"])\n",
    "merged_df.drop([\"Street Name\", \"Incident Zip\", \"Address\", \"ZipCode\", \"X Coord\", \"Y Coord\"], axis=1, inplace=True)\n",
    "merged_df[\"Complaint Made\"].fillna(0, inplace=True)\n",
    "merged_df[\"Complaint Made\"] = merged_df[\"Complaint Made\"].astype(int)\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the data into one dataframe, I started calculated the Pearson correlations and p-values of the features causing a complaint to be made. I outputed this as a seperate dataframe. To help visualise the Pearson correlation more easily, I also created a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6bc1648035f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcorr_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pearson Corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"P-Value\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpearson_coef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Complaint Made\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "corr_df = pd.DataFrame(columns=merged_df.columns[2:], index=[\"Pearson Corr\", \"P-Value\"])\n",
    "for column in corr_df.columns:\n",
    "    pearson_coef, p_value = stats.pearsonr(merged_df[\"Complaint Made\"], merged_df[column])\n",
    "    corr_df[column][\"Pearson Corr\"] = pearson_coef\n",
    "    corr_df[column][\"P-Value\"] = p_value\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_df = merged_df.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(pearson_df, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Approach\n",
    "For the second approach, I did the same process as with the first approach except instead of ones and zeros representing a complaint being made, a number was returned for the number of complaints each address had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_com_df = pd.DataFrame({\"Number of Complaints\" : bronx_heating_df.groupby([\"Incident Zip\", \"Street Name\"]).size()}).reset_index()\n",
    "num_com_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = pd.merge(num_com_df, house_df, how=\"right\", left_on=[\"Street Name\", \"Incident Zip\"], right_on=[\"Address\", \"ZipCode\"])\n",
    "merged_df2.drop([\"Street Name\", \"Incident Zip\", \"Address\", \"ZipCode\", \"X Coord\", \"Y Coord\"], axis=1, inplace=True)\n",
    "merged_df2[\"Number of Complaints\"].fillna(0, inplace=True)\n",
    "merged_df2[\"Number of Complaints\"] = merged_df2[\"Number of Complaints\"].astype(int)\n",
    "merged_df2[\"Number of Complaints\"].value_counts()\n",
    "merged_df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson Correlation, p-value, and correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df2 = pd.DataFrame(columns=merged_df2.columns[1:], index=[\"Pearson Corr\", \"P-Value\"])\n",
    "for column in corr_df2.columns:\n",
    "    pearson_coef, p_value = stats.pearsonr(merged_df2[\"Number of Complaints\"], merged_df2[column])\n",
    "    corr_df2[column][\"Pearson Corr\"] = pearson_coef\n",
    "    corr_df2[column][\"P-Value\"] = p_value\n",
    "\n",
    "corr_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_df2 = merged_df2.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(pearson_df2, annot=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
